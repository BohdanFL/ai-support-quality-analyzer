services:
    ollama:
        image: ollama/ollama:latest
        container_name: ollama-server
        volumes:
            - ollama_models:/root/.ollama
        ports:
            - "11434:11434"
        restart: unless-stopped

    llm_app:
        build: .
        container_name: llm_analytics
        depends_on:
            - ollama
        environment:
            - OLLAMA_HOST=http://ollama:11434
            - DEFAULT_MODEL=llama2
        volumes:
            - ./output:/app/output
            - .:/app
        stdin_open: true
        tty: true
        command: /bin/bash

volumes:
    ollama_models:
